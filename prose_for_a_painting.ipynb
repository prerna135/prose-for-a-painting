{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespeare_experiments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAMc7HwDWgno",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7esUwjXA1hvG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install mxnet colorama"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5UPvAj85aQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "# tf.set_random_seed(42)\n",
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras import backend as K\n",
        "import time\n",
        "import pickle\n",
        "import sys\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5WZVDbdW1xQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx19HyoNfRzj",
        "colab_type": "text"
      },
      "source": [
        "W have reused code from two Github repos: [Shakespearizing Modern English](https://github.com/harsh19/Shakespearizing-Modern-English/) and [Image2Poem](https://github.com/bei21/img2poem). Some minor changes to code have been made for experiment purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rzv7vKABIS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/gdrive', force_remount=True)\n",
        "# main_dir = '/gdrive/My Drive/Deep Learning Project/'\n",
        "main_dir = './'\n",
        "shakespeare_dir = 'Shakespearizing-Modern-English'\n",
        "img2poem_dir = 'img2poem/'\n",
        "data_dir_shakespeare = main_dir + shakespeare_dir + '/data/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcp4Ru9P0lua",
        "colab_type": "text"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33S1w9v65mSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# preprocessing params\n",
        "max_input_seq_length = 25\n",
        "max_output_seq_length = 25\n",
        "do_vocab_pruning = True\n",
        "max_vocab_size = 12000\n",
        "pretrained_embeddings_path = data_dir_shakespeare + 'embeddings/retrofitted_external_192_startend.p'\n",
        "embedding_dict = pickle.load(open(pretrained_embeddings_path, 'rb'), encoding='latin-1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgl6xUG--eWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unknown_word = \"UNK\".lower()\n",
        "sent_start = \"SENTSTART\".lower()\n",
        "sent_end = \"SENTEND\".lower()\n",
        "pad_word = \"PADWORD\".lower()\n",
        "special_tokens = [sent_start, sent_end, pad_word, unknown_word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMKYJQtXApKz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def init_vocab_items():\n",
        "  word_counters = {}\n",
        "  word_to_idx = {}\n",
        "  word_to_idx_ctr = 0 \n",
        "  idx_to_word = {}\n",
        "\n",
        "  word_to_idx[pad_word] = word_to_idx_ctr # 0 is for padword\n",
        "  idx_to_word[word_to_idx_ctr]= pad_word\n",
        "  word_counters[pad_word] = 1\n",
        "  word_to_idx_ctr += 1\n",
        "\n",
        "  word_to_idx[sent_start] = word_to_idx_ctr\n",
        "  word_counters[sent_start] = 1\n",
        "  idx_to_word[word_to_idx_ctr] = sent_start\n",
        "  word_to_idx_ctr += 1\n",
        "\n",
        "  word_to_idx[sent_end] = word_to_idx_ctr\n",
        "  word_counters[sent_end] = 1\n",
        "  idx_to_word[word_to_idx_ctr]= sent_end\t\t\n",
        "  word_to_idx_ctr += 1\n",
        "\n",
        "  word_counters[unknown_word] = 1\n",
        "  word_to_idx[unknown_word] = word_to_idx_ctr\n",
        "  idx_to_word[word_to_idx_ctr] = unknown_word\t\t\n",
        "  word_to_idx_ctr += 1\n",
        "\n",
        "  return word_counters, word_to_idx, word_to_idx_ctr, idx_to_word\n",
        "\n",
        "def load_vocab(split, word_counters, word_to_idx, word_to_idx_ctr, idx_to_word):\n",
        "  print('Load', split, 'data')\n",
        "  inp_src = data_dir_shakespeare + split + '.original.nltktok' #modern\n",
        "  out_src = data_dir_shakespeare + split + '.modern.nltktok' #original\n",
        "  inp_data = open(inp_src, 'r').readlines()\n",
        "  out_data = open(out_src, 'r').readlines()\n",
        "  inputs = [row.strip().lower().split(' ') for row in inp_data]\n",
        "  outputs = [row.strip().lower().split(' ') for row in out_data]\n",
        "  for text in inputs:\n",
        "    for token in text:\n",
        "      if token not in word_to_idx:\n",
        "        word_to_idx[token] = word_to_idx_ctr\n",
        "        idx_to_word[word_to_idx_ctr] = token\n",
        "        word_to_idx_ctr += 1\n",
        "        word_counters[token] = 0\n",
        "      word_counters[token] += 1\n",
        "  for text in outputs:\n",
        "    for token in text:\n",
        "      if token not in word_to_idx:\n",
        "        word_to_idx[token] = word_to_idx_ctr\n",
        "        idx_to_word[word_to_idx_ctr] = token\n",
        "        word_to_idx_ctr += 1\n",
        "        word_counters[token] = 0\n",
        "      word_counters[token] += 1\n",
        "  vocab_size = len(word_to_idx)\n",
        "  return vocab_size\n",
        "\n",
        "def prune_vocab(max_vocab_size, word_counters, word_to_idx, word_to_idx_ctr, idx_to_word):\n",
        "  tmp_word_counters, tmp_word_to_idx, tmp_word_to_idx_ctr, tmp_idx_to_word = init_vocab_items()\n",
        "  print('Vocab size before pruning:', len(word_to_idx))\n",
        "  top_items = sorted(word_counters.items(), key=lambda x:-x[1])[:max_vocab_size]\n",
        "  for token_count in top_items:\n",
        "    token = token_count[0]\n",
        "    if token in special_tokens:\n",
        "      continue\n",
        "    tmp_word_to_idx[token] = tmp_word_to_idx_ctr\n",
        "    tmp_idx_to_word[tmp_word_to_idx_ctr] = token\n",
        "    tmp_word_to_idx_ctr += 1\n",
        "  word_to_idx = tmp_word_to_idx\n",
        "  idx_to_word = tmp_idx_to_word\n",
        "  vocab_size = len(tmp_word_to_idx)\n",
        "  word_to_idx_ctr = tmp_word_to_idx_ctr\n",
        "  print('Vocab size after pruning:', vocab_size)\n",
        "  return word_counters, word_to_idx, word_to_idx_ctr, idx_to_word, vocab_size\n",
        "\n",
        "def idxseq_to_vocabseq(seq):\n",
        "\treturn [idx_to_word[x] for x in seq]\n",
        "\n",
        "def load_gan_data(split):\n",
        "  print('Load', split, 'data')\n",
        "  inp_src = data_dir_shakespeare + split + '.modern.nltktok' #modern\n",
        "  out_src = data_dir_shakespeare + split + '.original.nltktok' #original\n",
        "  inp_data = open(inp_src, 'r').readlines()\n",
        "  out_data = open(out_src, 'r').readlines()\n",
        "  data0 = [row.strip().lower().split(' ') for row in inp_data]\n",
        "  data1 = [row.strip().lower().split(' ') for row in out_data]\n",
        "  enc_ip0 = [] \t\t\n",
        "  dec_ip0 = []\n",
        "  dec_op0 = []\n",
        "  enc_ip1 = [] \t\t\n",
        "  dec_ip1 = []\n",
        "  dec_op1 = []\n",
        "  for text in data0:\n",
        "    sent = [word_to_idx[w] if w in word_to_idx else word_to_idx[unknown_word] for w in text]\n",
        "    enc_ip0.append(sent[::-1])\n",
        "    dec_ip0.append([word_to_idx[sent_start]] + sent)\n",
        "    dec_op0.append(sent + [word_to_idx[sent_end]])\n",
        "  lens = [len(entry) for entry in dec_ip0]\n",
        "  weights0 = [[1.0] * min(entry,max_input_seq_length+1) + [0.0] * (max_input_seq_length+1-entry) for entry in lens]\n",
        "  enc_ip0 = pad_sequences(enc_ip0, max_input_seq_length, padding='pre', truncating='post')\n",
        "  dec_ip0 = pad_sequences(dec_ip0, max_input_seq_length+1, padding='post', truncating='post')\n",
        "  dec_op0 = pad_sequences(dec_op0, max_input_seq_length+1, padding='post', truncating='post')\n",
        "  for text in data1:\n",
        "    sent = [word_to_idx[w] if w in word_to_idx else word_to_idx[unknown_word] for w in text]\n",
        "    enc_ip1.append(sent[::-1])\n",
        "    dec_ip1.append([word_to_idx[sent_start]] + sent)\n",
        "    dec_op1.append(sent + [word_to_idx[sent_end]])\n",
        "  lens = [len(entry) for entry in dec_ip1]\n",
        "  weights1 = [[1.0] * min(entry,max_input_seq_length+1) + [0.0] * (max_input_seq_length+1-entry) for entry in lens]\n",
        "  enc_ip1 = pad_sequences(enc_ip1, max_input_seq_length, padding='pre', truncating='post')\n",
        "  dec_ip1 = pad_sequences(dec_ip1, max_input_seq_length+1, padding='post', truncating='post')\n",
        "  dec_op1 = pad_sequences(dec_op1, max_input_seq_length+1, padding='post', truncating='post')\n",
        "  return enc_ip0, dec_ip0, dec_op0, enc_ip1, dec_ip1, dec_op1, np.array(weights0).astype(np.float32), np.array(weights1).astype(np.float32)\n",
        "\n",
        "def load_data(split):\n",
        "  print('Load', split, 'data')\n",
        "  inp_src = data_dir_shakespeare + split + '.modern.nltktok' #modern\n",
        "  out_src = data_dir_shakespeare + split + '.original.nltktok' #original\n",
        "  inp_data = open(inp_src, 'r').readlines()\n",
        "  out_data = open(out_src, 'r').readlines()\n",
        "  inputs = [row.strip().lower().split(' ') for row in inp_data]\n",
        "  outputs = [row.strip().lower().split(' ') for row in out_data]\n",
        "  # generate sequences\n",
        "  sequences_input = [] \t\t\n",
        "  sequences_output = []\n",
        "  sequences_input_lens = []\n",
        "  sequences_output_lens = []\n",
        "  for text in inputs:\n",
        "    tmp = [word_to_idx[sent_start]]\n",
        "    for token in text:\n",
        "      if token not in word_to_idx:\n",
        "        tmp.append(word_to_idx[unknown_word])\n",
        "      else:\n",
        "        tmp.append(word_to_idx[token])\n",
        "    tmp.append(word_to_idx[sent_end])\n",
        "    sequences_input.append(tmp)\n",
        "  for text in outputs:\n",
        "    tmp = [word_to_idx[sent_start]]\n",
        "    for token in text:\n",
        "      if token not in word_to_idx:\n",
        "        tmp.append(word_to_idx[unknown_word])\n",
        "      else:\n",
        "        tmp.append(word_to_idx[token])\n",
        "    tmp.append(word_to_idx[sent_end])\n",
        "    sequences_output.append(tmp)\n",
        "  sequences_input_lens = [len(ip) for ip in sequences_input]\n",
        "  sequences_output_lens = [len(op) for op in sequences_output]\n",
        "  # pad sequences\n",
        "  sequences_input = pad_sequences(sequences_input, max_input_seq_length, padding='pre', truncating='post')\n",
        "  sequences_output = pad_sequences(sequences_output, max_output_seq_length, padding='post', truncating='post')\n",
        "\n",
        "  print('Printing sample sequence:')\n",
        "  print(sequences_input[0], ':', idxseq_to_vocabseq(sequences_input[0]), \"---\", sequences_output[0], \":\", idxseq_to_vocabseq(sequences_output[0]))\n",
        "  return sequences_input, sequences_output, sequences_input_lens, sequences_output_lens\n",
        "\n",
        "def prepare_data(sequences, seed=42, shuffle=False):\n",
        "  inputs, outputs, _, _ = sequences\n",
        "  decoder_inputs = np.array([sequence[:-1] for sequence in outputs])\n",
        "  decoder_outputs = np.array([sequence[1:] for sequence in outputs])\n",
        "  matching_input_token = []\n",
        "  for cur_outputs, cur_inputs in zip(decoder_outputs, inputs):\n",
        "    tmp = []\n",
        "    for output_token in cur_outputs:\n",
        "      idx = np.zeros(len(cur_inputs), dtype=np.float32)\n",
        "      for j, token in enumerate(cur_inputs):\n",
        "        if token <= 3:\n",
        "          continue\n",
        "        if token == output_token:\n",
        "          idx[j] = 1.0\n",
        "      tmp.append(idx)\n",
        "    matching_input_token.append(tmp)\n",
        "  matching_input_token = np.array(matching_input_token)\n",
        "  encoder_inputs = np.array(inputs)\n",
        "  if shuffle:\n",
        "    indices = np.arange(encoder_inputs.shape[0])\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(indices)\n",
        "  return encoder_inputs, decoder_inputs, decoder_outputs, matching_input_token"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUMF-aGnClus",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counters, word_to_idx, word_to_idx_ctr, idx_to_word = init_vocab_items()\n",
        "splits = ['train', 'valid', 'test']\n",
        "vocab_size = load_vocab('train', word_counters, word_to_idx, word_to_idx_ctr, idx_to_word)\n",
        "if do_vocab_pruning is True:\n",
        "  word_counters, word_to_idx, word_to_idx_ctr, idx_to_word, vocab_size = prune_vocab(max_vocab_size, word_counters, word_to_idx, word_to_idx_ctr, idx_to_word)\n",
        "data_seq = {split : load_data(split) for split in splits}\n",
        "data = {split : prepare_data(cur_data) for split, cur_data in data_seq.items()}\n",
        "for split, split_data in data.items():\n",
        "  inp, dinp, dout, dout_inp_matches = split_data\n",
        "# data_seq = {split : load_gan_data(split) for split in splits}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAVnDslFkho8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = np.zeros((max_vocab_size, 192), dtype=np.float32)\n",
        "for word in word_to_idx:\n",
        "  if word in embedding_dict:\n",
        "    embeddings[word_to_idx[word]] = embedding_dict[word]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpAgj1ghMLO3",
        "colab_type": "text"
      },
      "source": [
        "**Cross Aligned Auto-Encoder (GAN)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-HxqT8MMKhj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class model(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, dim_emb, dim_y, dim_z, n_layers, max_seq_len, filter_sizes,\n",
        "              n_filters, dropout, embeddings, batch_size):\n",
        "    super(model, self).__init__()\n",
        "    self.dim_y = dim_y # style\n",
        "    self.dim_z = dim_z # context\n",
        "    self.dim_h = dim_y + dim_z\n",
        "    self.dim_emb = dim_emb\n",
        "    self.n_layers = n_layers\n",
        "    self.max_len = max_seq_len\n",
        "    self.filter_sizes = [int(x) for x in filter_sizes.split(',')]\n",
        "    self.n_filters = n_filters\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = 0.1\n",
        "    self.rho = 1.0\n",
        "    self.vocab_size = vocab_size\n",
        "    self.dropout = dropout\n",
        "    self.dl1 = tf.keras.layers.Dense(self.vocab_size)\n",
        "    self.dl2 = tf.keras.layers.Dense(self.dim_y)\n",
        "    self.dl3 = tf.keras.layers.Dense(1)\n",
        "    self.dl4 = tf.keras.layers.Dense(1)\n",
        "    self.embeddings = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=dim_emb)\n",
        "    # self.embeddings = embeddings\n",
        "    self.gru_enc = tf.keras.layers.GRU(self.dim_h, dropout=dropout,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='he_normal')\n",
        "    self.gru_dec = tf.keras.layers.GRU(self.dim_h, dropout=dropout,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='he_normal')\n",
        "    self.cnn1 = [] # for discriminator D1\n",
        "    self.cnn2 = [] # for discriminator D2\n",
        "    for size in self.filter_sizes:\n",
        "      self.cnn1.append([tf.keras.layers.Conv1D(n_filters, size, strides=1, padding='same'), tf.keras.layers.LeakyReLU()])\n",
        "      self.cnn2.append([tf.keras.layers.Conv1D(n_filters, size, strides=1, padding='same'), tf.keras.layers.LeakyReLU()])\n",
        "    self.max_pool = tf.keras.layers.MaxPooling1D()\n",
        "\n",
        "\n",
        "  def call(self, enc_inp, dec_inp, targets, labels, weights, hidden):\n",
        "    # encoder\n",
        "#     enc_inp = tf.nn.embedding_lookup(self.embeddings, enc_inp)\n",
        "#     dec_inp = tf.nn.embedding_lookup(self.embeddings, dec_inp)\n",
        "    enc_inp = self.embeddings(enc_inp)\n",
        "    dec_inp = self.embeddings(dec_inp)\n",
        "    _, z = self.gru_enc(enc_inp, initial_state=hidden)\n",
        "    # latent content representation of original sentence, basically labels represent style\n",
        "    z = z[:, self.dim_y:] #(batch_size, dim_z)\n",
        "    h_ori = tf.concat([self.dl2(labels), z], 1) #(batch_size, dim_h) content + original style\n",
        "    h_tsf = tf.concat([self.dl2(1 - labels), z], 1) #(batch_size, dim_h) content + new style\n",
        "    \n",
        "    #decoder\n",
        "    g_out, _ = self.gru_dec(dec_inp, initial_state=h_ori) #(batch_size, max_len, dim_h)\n",
        "    teach_h = tf.concat([tf.expand_dims(h_ori, 1), g_out], 1) #(batch_size, max_len+1, dim_h)\n",
        "    g_out = tf.nn.dropout(g_out, self.dropout)\n",
        "    # g_out = tf.reshape(g_out, [-1, self.dim_h]) #(batch_size * (max_len+1), dim_h)\n",
        "    g_logits = self.dl1(g_out)\n",
        "    \n",
        "    go = dec_inp[:, :1, :] #start_token (batch_size, 1, dim_emb)\n",
        "    soft_h_ori, soft_logits_ori = self.decode(h_ori, go, self.softmax_word())\n",
        "    soft_h_tsf, soft_logits_tsf = self.decode(h_tsf, go, self.softmax_word())\n",
        "    self.hard_h_ori, self.hard_logits_ori = self.decode(h_ori, go, self.argmax_word())\n",
        "    self.hard_h_tsf, self.hard_logits_tsf = self.decode(h_tsf, go, self.argmax_word())\n",
        "\n",
        "    if targets is not None:\n",
        "      loss_rec = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(targets, [-1]), logits=g_logits)\n",
        "      loss_rec *= tf.reshape(weights, [-1])\n",
        "      self.loss_rec = tf.reduce_sum(loss_rec) / self.batch_size\n",
        "\n",
        "      # discriminators D1 and D2\n",
        "      # D1: given style y_1 and contents z_1 and z_2\n",
        "      # D2: given style y_2 and contents z_1 and z_2\n",
        "      # soft_h_tsf is hidden representation of content + transfered style\n",
        "      # teach_h is content + original style\n",
        "      half = int(self.batch_size / 2)\n",
        "      zeros, ones = labels[:half], labels[half:]\n",
        "      d0 = self.cnn_f(teach_h[:half], self.cnn1, self.dl3)\n",
        "      g0 = self.cnn_f(soft_h_tsf[half:], self.cnn1, self.dl3)\n",
        "      d1 = self.cnn_f(teach_h[half:], self.cnn2, self.dl4)\n",
        "      g1 = self.cnn_f(soft_h_tsf[:half], self.cnn2, self.dl4)\n",
        "\n",
        "      self.loss_d0 = tf.reduce_mean(\n",
        "          tf.nn.sigmoid_cross_entropy_with_logits(labels=ones, logits=d0)) + tf.reduce_mean(\n",
        "          tf.nn.sigmoid_cross_entropy_with_logits(labels=zeros, logits=g0))\n",
        "      loss_g0 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=ones, logits=g0))\n",
        "      self.loss_d1 = tf.reduce_mean(\n",
        "          tf.nn.sigmoid_cross_entropy_with_logits(labels=ones, logits=d1)) + tf.reduce_mean(\n",
        "          tf.nn.sigmoid_cross_entropy_with_logits(labels=zeros, logits=g1))\n",
        "      loss_g1 = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=ones, logits=g1))\n",
        "\n",
        "      self.loss_adv = loss_g0 + loss_g1\n",
        "      self.loss = self.loss_rec + self.rho * self.loss_adv\n",
        "      return self.loss_rec, self.loss_adv, self.loss, self.loss_d0, self.loss_d1\n",
        "\n",
        "  def initialize_hidden_state(self, labels):\n",
        "    return tf.concat([self.dl2(labels), tf.zeros([self.batch_size, self.dim_z], dtype=tf.float32)], 1) #(batch_size, dim_h)\n",
        "  \n",
        "  def softmax_word(self):\n",
        "    def loop_func(output):\n",
        "        output = tf.nn.dropout(output, self.dropout)\n",
        "        logits = self.dl1(output)\n",
        "        prob = tf.nn.softmax(logits / self.gamma)\n",
        "        inp = tf.matmul(prob, self.embeddings.get_weights()[0])\n",
        "        return inp, logits\n",
        "    return loop_func\n",
        "\n",
        "  def argmax_word(self):\n",
        "    def loop_func(output):\n",
        "        output = tf.nn.dropout(output, self.dropout)\n",
        "        logits = self.dl1(output)\n",
        "        word = tf.argmax(logits, axis=1)\n",
        "#         inp = tf.nn.embedding_lookup(self.embeddings, word)\n",
        "        inp = self.embeddings(word)\n",
        "        return inp, logits\n",
        "    return loop_func\n",
        "\n",
        "  def decode(self, hidden, inp, loop_func):\n",
        "    h_seq, logits_seq = [], []\n",
        "    for i in range(self.max_len):\n",
        "      h_seq.append(tf.expand_dims(hidden, 1)) #[(batch_size, 1, dim_h)]\n",
        "      output, hidden = self.gru_dec(inp, initial_state=hidden)\n",
        "      inp, logits = loop_func(output)\n",
        "      inp = tf.expand_dims(inp, 1)\n",
        "      logits_seq.append(tf.expand_dims(logits, 1))\n",
        "    return tf.concat(h_seq, 1), tf.concat(logits_seq, 1) # (batch_size, max_len, dim_h), (batch_size, max_len, vocab_size)\n",
        "  \n",
        "  def cnn_f(self, inp, cnn, dl):\n",
        "    outputs = []\n",
        "    for i in range(len(cnn)):\n",
        "      op = cnn[i][0](inp)\n",
        "      op = cnn[i][1](op)\n",
        "      op = self.max_pool(op)\n",
        "      outputs.append(tf.keras.layers.Flatten()(op))\n",
        "    outputs = tf.concat(outputs, 1)\n",
        "    outputs = tf.nn.dropout(outputs, self.dropout)\n",
        "    logits = dl(outputs)\n",
        "    return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdl41vDUKPSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "optimizer = tf.keras.optimizers.Adam(0.0005, 0.5, 0.999)\n",
        "m = model(max_vocab_size, dim_emb=256, dim_y=256, dim_z=512, n_layers=1, max_seq_len=max_input_seq_length+1, filter_sizes='1,2,3',\n",
        "          n_filters=128, dropout=0.5, embeddings=embeddings, batch_size=batch_size*2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMYlI0rpMJE6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_epochs = 20\n",
        "n_iters = int(len(data_seq['train'][0]) / batch_size) - 1\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  shuffled_idx = np.arange(len(data_seq['train'][0]))\n",
        "  np.random.shuffle(shuffled_idx)\n",
        "  for it in range(n_iters):\n",
        "    batch_idx = shuffled_idx[it*batch_size:(it+1)*batch_size]\n",
        "    batch_enc_ip = tf.concat([data_seq['train'][0][batch_idx], data_seq['train'][3][batch_idx]], 0)\n",
        "    batch_dec_ip = tf.concat([data_seq['train'][1][batch_idx], data_seq['train'][4][batch_idx]], 0)\n",
        "    batch_dec_op = tf.concat([data_seq['train'][2][batch_idx], data_seq['train'][5][batch_idx]], 0)\n",
        "    batch_weights = tf.concat([data_seq['train'][6][batch_idx], data_seq['train'][7][batch_idx]], 0)\n",
        "\n",
        "    labels = [0.0] * batch_size + [1.0] * batch_size\n",
        "    labels = tf.reshape(labels, [-1, 1])\n",
        "    hidden = m.initialize_hidden_state(labels)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      loss_rec, loss_adv, loss, loss_d0, loss_d1 = m(batch_enc_ip, batch_dec_ip, batch_dec_op, labels, batch_weights, hidden)\n",
        "    vars_ = m.gru_enc.trainable_variables + m.gru_dec.trainable_variables + m.dl1.trainable_variables + m.dl2.trainable_variables + m.embeddings.trainable_variables\n",
        "    d0_vars = m.cnn1.trainable_variables + m.dl3.trainable_variables\n",
        "    d1_vars = m.cnn2.trainable_variables + m.dl4.trainable_variables\n",
        "    grad_d0 = tape.gradient(loss_d0, d0_vars)\n",
        "    optimizer.apply_gradients(zip(grad_d0, d0_vars))\n",
        "    grad_d1 = tape.gradient(loss_d1, d1_vars)\n",
        "    optimizer.apply_gradients(zip(grad_d1, d1_vars))\n",
        "    if loss_d0.numpy() < 1.2 and loss_d1.numpy() < 1.2:\n",
        "      grad = tape.gradient(loss, vars_)\n",
        "      optimizer.apply_gradients(zip(grad, vars_))\n",
        "    else:\n",
        "      grad_rec = tape.gradient(loss_rec, vars_)\n",
        "      optimizer.apply_gradients(zip(grad_rec, vars_))\n",
        "    if it % 50 == 0:\n",
        "      print('Epoch:', epoch, 'iter:', it, 'loss_rec:', loss_rec.numpy(), 'loss_adv:',\n",
        "            loss_adv.numpy(), 'loss:', loss.numpy(), 'loss_d0:', loss_d0.numpy(), 'loss_d1:', loss_d1.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNxLtgsgqQyc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def strip_eos(sents):\n",
        "  return [sent[:sent.index(sent_end)] if sent_end in sent else sent for sent in sents]\n",
        "\n",
        "def greedy_decoding(enc_ip, dec_ip, weights):\n",
        "  batch_size = enc_ip.shape[0]\n",
        "  labels = [0.0] * batch_size\n",
        "  labels = tf.reshape(labels, [-1, 1])\n",
        "  hidden = tf.concat([m.dl2(labels), tf.zeros([batch_size, m.dim_z], dtype=tf.float32)], 1)\n",
        "  m(enc_ip, dec_ip, None, labels, weights, hidden)\n",
        "  ori = np.argmax(m.hard_logits_ori, axis=2).tolist()\n",
        "  ori = [[idx_to_word[i] for i in sent] for sent in ori]\n",
        "  tsf = np.argmax(m.hard_logits_tsf, axis=2).tolist()\n",
        "  tsf = [[idx_to_word[i] for i in sent] for sent in tsf]\n",
        "  strip_eos(ori)\n",
        "  strip_eos(tsf)\n",
        "  ori = [' '.join(sent) for sent in ori]\n",
        "  tsf = [' '.join(sent) for sent in tsf]\n",
        "  return ori, tsf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owD5xRra6r0F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_enc_ip = tf.concat([data_seq['test'][0][:10], data_seq['test'][3][:10]], 0)\n",
        "batch_dec_ip = tf.concat([data_seq['test'][1][:10], data_seq['test'][4][:10]], 0)\n",
        "# batch_dec_op = tf.concat([data_seq['test'][2][:10], data_seq['test'][5][:10]], 0)\n",
        "batch_weights = tf.concat([data_seq['test'][6][:10], data_seq['test'][7][:10]], 0)\n",
        "ori, tsf = greedy_decoding(batch_enc_ip, batch_dec_ip, batch_weights)\n",
        "print(ori)\n",
        "print('-------------------------------------------------------------------------')\n",
        "print(tsf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmQfOAUW0rcl",
        "colab_type": "text"
      },
      "source": [
        "**Pointer Model and Local Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5gw97uBWK_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(data_seq['train'][0])\n",
        "BATCH_SIZE = 128\n",
        "steps_per_epoch = len(data_seq['train'][0])//BATCH_SIZE\n",
        "embedding_dim = 192\n",
        "units = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdKC9b-YX7j4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor_train = tf.convert_to_tensor(data_seq['train'][0])\n",
        "target_tensor_train = tf.convert_to_tensor(data_seq['train'][1])\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1_oL1NRPetK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pointer Model\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz, embeddings):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embeddings], trainable=False)\n",
        "    # self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units, dropout=0.25,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='he_normal')\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    hidden_with_time_axis = tf.expand_dims(query, 1) #(batch_size, 1, hidden_size)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(values) + self.W2(hidden_with_time_axis))) #(batch_size, maxlen+1, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1) #(batch_size, hidden_size)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class PointerAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(PointerAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units, activation='tanh')\n",
        "\n",
        "  def call(self, h_prev, encoder_vals):\n",
        "    units = h_prev.get_shape().as_list()[-1]\n",
        "    batch_size = h_prev.get_shape().as_list()[0]\n",
        "    sentinel = tf.random.uniform(shape=(batch_size, 1, units))\n",
        "    encoder_vals_expanded = tf.concat([sentinel, encoder_vals], axis=1)\n",
        "    query = self.W1(h_prev)\n",
        "    h_att = tf.expand_dims(query, 1) # (batch_size, 1, units)\n",
        "    out_att = tf.reduce_sum(tf.multiply(h_att, encoder_vals_expanded), axis=2) # (batch_size, max_length + 1)\n",
        "    alpha = tf.nn.softmax(out_att) # (batch_size, max_length + 1)\n",
        "    sentinel_weights = alpha[:, 0]\n",
        "    alpha = alpha[:, 1:]\n",
        "    context = tf.reduce_sum(encoder_vals * tf.expand_dims(alpha, 2), 1)   #(batch_size, units)\n",
        "    return alpha, sentinel_weights, context\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, embeddings):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embeddings], trainable=False)\n",
        "    # self.embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units, dropout=0.25,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='he_normal')\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "    self.fc_context = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    # self.attention = BahdanauAttention(self.dec_units)\n",
        "    # pointer model\n",
        "    self.attention = PointerAttention(self.dec_units)\n",
        "    \n",
        "\n",
        "  def call(self, x, hidden, enc_output, encoder_input):\n",
        "  # def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    # for local attention\n",
        "    # context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "    encoder_length = tf.shape(enc_output)[1]\n",
        "    batch_size = tf.shape(enc_output)[0]\n",
        "    # for pointer attention\n",
        "    alpha, sentinel_weights, context_vector = self.attention(hidden, enc_output)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "    output, state = self.gru(x)\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output) # without pointer model\n",
        "    x = self.fc(output) + self.fc_context(context_vector)\n",
        "\n",
        "    # For pointer model\n",
        "    pred_softmax = tf.nn.softmax(x) #(batch_size, vocab_size)\n",
        "    sentinel_weights = tf.expand_dims(sentinel_weights, 1) #(batch_size, 1)\n",
        "    pred = pred_softmax * sentinel_weights #(batch_size, vocab_size)\n",
        "    r = tf.expand_dims(tf.range(batch_size), 1) #(batch_size, 1)\n",
        "    r = tf.tile(r, [1, encoder_length]) # (batch_size, encoder_length)\n",
        "    r_concat = tf.stack([r, encoder_input], axis=2) # (batch_size, encoder_length, 2)\n",
        "    r_concat_flattened = tf.reshape(r_concat, [-1, 2]) # (batch_size * encoder_length, 2)\n",
        "    r_concat_flattened = tf.cast(r_concat_flattened, tf.int64)\n",
        "    alpha_flattened = tf.reshape(alpha, [-1]) # (batch_size * encoder_length)\n",
        "    dense_shape = np.array([batch_size, self.vocab_size], dtype=np.int64)\n",
        "    pointer_probs = tf.SparseTensor(indices=r_concat_flattened, values=alpha_flattened, dense_shape=dense_shape)\n",
        "    x = tf.sparse.add(pred, pointer_probs)\n",
        "\n",
        "    # return x, state, attention_weights # for local attention\n",
        "    return x, state, alpha, sentinel_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOaEWodNVxVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = Encoder(max_vocab_size, embedding_dim, units, BATCH_SIZE, embeddings)\n",
        "decoder = Decoder(max_vocab_size, embedding_dim, units, BATCH_SIZE, embeddings)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4PBf-X2UXgC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyFle3VlUf3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# @tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([word_to_idx[sent_start]] * BATCH_SIZE, 1)\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "      # predictions, dec_hidden, _, _ = decoder(dec_input, dec_hidden, enc_output, inp)\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePvKGyeBVQQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 25\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYYkLhqkxafz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = data_dir_shakespeare + 'training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)\n",
        "checkpoint.save(file_prefix = checkpoint_prefix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fiz2WYN0a0ij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(text):\n",
        "  attention_plot = np.zeros((max_output_seq_length, max_input_seq_length))\n",
        "  tmp = [word_to_idx[sent_start]]\n",
        "  for token in text:\n",
        "    if token not in word_to_idx:\n",
        "      tmp.append(word_to_idx[unknown_word])\n",
        "    else:\n",
        "      tmp.append(word_to_idx[token])\n",
        "  tmp.append(word_to_idx[sent_end])\n",
        "  tmp = pad_sequences([tmp], max_input_seq_length, padding='pre', truncating='post')\n",
        "  tmp = tf.convert_to_tensor(tmp)\n",
        "  result = ''\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(tmp, hidden)\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([word_to_idx[sent_start]], 0)\n",
        "\n",
        "  for t in range(max_output_seq_length):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
        "    # predictions, dec_hidden, attention_weights, sentinel_weights = decoder(dec_input, dec_hidden, enc_out, tmp)\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    sampled_word = idx_to_word[predicted_id]\n",
        "    if predicted_id == word_to_idx[unknown_word]:\n",
        "      predicted_id_ = np.argmax(attention_weights)\n",
        "      sampled_word = idx_to_word[int(tmp[0][predicted_id_])]\n",
        "      if sampled_word != unknown_word and sampled_word != sent_start and sampled_word != pad_word and sampled_word != sent_end:\n",
        "        result += sampled_word + ' '\n",
        "    elif sampled_word != sent_start and sampled_word != pad_word and sampled_word != sent_end:\n",
        "      result += sampled_word + ' '\n",
        "    if sampled_word == sent_end:\n",
        "      return result, text, attention_plot\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, text, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqYy4PGkcUdD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "  fontdict = {'fontsize': 14}\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtaOwkWrcZLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "  return result, attention_plot\n",
        "#   print('Input: %s' % (sentence))\n",
        "#   print('Predicted translation: {}'.format(result))\n",
        "#   attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "#   plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iliBnndWR3U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Post-processing\n",
        "def post_process(input_file, output_file):\n",
        "  input_file = open(input_file)\n",
        "  input_lines = input_file.readlines()\n",
        "  input_lines = [row.strip().lower().split(' ') for row in input_lines]\n",
        "  sequences_output = []\n",
        "  with open(output_file, 'w') as f:\n",
        "    for i, line in enumerate(input_lines):\n",
        "      output, _ = translate(line)\n",
        "      sequences_output.append(output)\n",
        "      print(line, '\\n', output, '\\n', idxseq_to_vocabseq(data_seq['test'][1][i]), '\\n-----------------------\\n')\n",
        "      f.write(output + '\\n')\n",
        "  return sequences_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98j4cnwPR2L1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_out = post_process(data_dir_shakespeare + 'test.modern.nltktok', data_dir_shakespeare + 'gmodel1.test.out')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y14QVc3bECN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test BLEU\n",
        "! perl /gdrive/My\\ Drive/Deep\\ Learning\\ Project/Shakespearizing-Modern-English/code/main/multi-bleu.perl -lc /gdrive/My\\ Drive/Deep\\ Learning\\ Project/Shakespearizing-Modern-English/data/test.original.nltktok < /gdrive/My\\ Drive/Deep\\ Learning\\ Project/Shakespearizing-Modern-English/data/gmodel1.test.out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmulMGpI0yGG",
        "colab_type": "text"
      },
      "source": [
        "**Seq2Seq With Global Attention**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPLeujpyhewW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Baseline Model with teacher forcing\n",
        "encoder_ip = Input(shape=(None, ))\n",
        "embed = Embedding(input_dim=max_vocab_size, output_dim=192, weights=[embeddings], trainable=False)\n",
        "encoder_embed = embed(encoder_ip)\n",
        "encoder_lstm = Bidirectional(LSTM(units=512, return_sequences=True, recurrent_dropout=0.25, dropout=0.25,\n",
        "                        kernel_initializer='he_normal', recurrent_activation='tanh', return_state=True))\n",
        "encoder, state_h_f, state_c_f, state_h_b, state_c_b = encoder_lstm(encoder_embed)\n",
        "encoder_states = [Concatenate()([state_h_f, state_h_b]), Concatenate()([state_c_f, state_c_b])]\n",
        "encoder = BatchNormalization()(encoder)\n",
        "\n",
        "decoder_ip = Input(shape=(None, ))\n",
        "decoder_embed = embed(decoder_ip)\n",
        "decoder_lstm = LSTM(units=1024, recurrent_dropout=0.25, dropout=0.25, return_sequences=True,\n",
        "                        kernel_initializer='he_normal',recurrent_activation='tanh', return_state=True)\n",
        "decoder, _, _ = decoder_lstm(decoder_embed, initial_state=encoder_states)\n",
        "attention_dot = dot([decoder, encoder], axes=[2, 2])\n",
        "attention_softmax = Activation('softmax')\n",
        "attention = attention_softmax(attention_dot)\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "decoder_combined_context = BatchNormalization()(decoder_combined_context)\n",
        "output_layer = Dense(max_vocab_size, activation='softmax')\n",
        "output = output_layer(decoder_combined_context)\n",
        "model_final = tf.keras.Model([encoder_ip, decoder_ip], output)\n",
        "# model_final.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMT20-8e79lL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inference model\n",
        "encoder_model = tf.keras.Model(encoder_ip, [encoder] + encoder_states)\n",
        "decoder_state_input_h = Input(shape=(1024,))\n",
        "decoder_state_input_c = Input(shape=(1024,))\n",
        "encoder_op = Input(shape=(None, 1024))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_op, d_s_h, d_s_c = decoder_lstm(embed(decoder_ip), initial_state=decoder_states_inputs)\n",
        "decoder_states = [d_s_h, d_s_c]\n",
        "inf_attention = attention_softmax(dot([decoder_op, encoder_op], axes=[2, 2]))\n",
        "inf_context = dot([inf_attention, encoder_op], axes=[2,1])\n",
        "decoder_op = concatenate([inf_context, decoder_op])\n",
        "decoder_op = output_layer(BatchNormalization()(decoder_op))\n",
        "decoder_model = tf.keras.Model(\n",
        "    [decoder_ip] + [encoder_op] + decoder_states_inputs,\n",
        "    [decoder_op] + decoder_states + [inf_attention])\n",
        "# decoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "892hqC_-cmZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_final.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "model_final.fit([data['train'][0], data['train'][1]], data['train'][2],\n",
        "          batch_size=32,\n",
        "          epochs=10, validation_data=([data['valid'][0], data['valid'][1]], data['valid'][2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOyYi9f41lJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "  encoder_op, e_h, e_c = encoder_model.predict(input_seq)\n",
        "  states_value = [e_h, e_c]\n",
        "  target_seq = np.zeros((1, 1), dtype=np.int32)\n",
        "  target_seq[0][0] = word_to_idx[sent_start]\n",
        "  stop_condition = False\n",
        "  decoded_sentence = []\n",
        "  cnt = 0\n",
        "  att_all = []\n",
        "  while stop_condition is not True:\n",
        "    output_tokens, h, c, att = decoder_model.predict([target_seq] + [encoder_op] + states_value)\n",
        "    att_all.append(att)\n",
        "    sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "    sampled_word = idx_to_word[sampled_token_index]\n",
        "    if cnt >= max_output_seq_length or len(decoded_sentence) >= max_output_seq_length or sampled_word == sent_end or sampled_word == pad_word:\n",
        "      stop_condition = True\n",
        "    elif sampled_word == unknown_word:\n",
        "      cnt += 1\n",
        "      sampled_token_index = np.argmax(att, axis=2)[0][0]\n",
        "      sampled_word = idx_to_word[input_seq[0][sampled_token_index]]\n",
        "      if sampled_word == pad_word or sampled_word == sent_end:\n",
        "        stop_condition = True\n",
        "      elif sampled_word != sent_start and sampled_word != unknown_word and sampled_word != \"\\\"de\":\n",
        "        decoded_sentence.append(sampled_word)\n",
        "    else:\n",
        "      cnt += 1\n",
        "      if sampled_word != \"\\\"de\":\n",
        "        decoded_sentence.append(sampled_word)\n",
        "    target_seq = np.zeros((1, 1), dtype=np.int32)\n",
        "    target_seq[0][0] = sampled_token_index\n",
        "    states_value = [h, c]\n",
        "  return ' '.join(decoded_sentence), att_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGe-teebMsp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # Post-processing\n",
        "def post_process(input_file, output_file):\n",
        "  input_file = open(input_file)\n",
        "  input_lines = input_file.readlines()\n",
        "  input_lines = [row.strip().lower().split(' ') for row in input_lines]\n",
        "  sequences_input = []\n",
        "  for text in input_lines:\n",
        "    tmp = [word_to_idx[sent_start]]\n",
        "    for token in text:\n",
        "      if token not in word_to_idx:\n",
        "        tmp.append(word_to_idx[unknown_word])\n",
        "      else:\n",
        "        tmp.append(word_to_idx[token])\n",
        "    tmp.append(word_to_idx[sent_end])\n",
        "    sequences_input.append(tmp)\n",
        "  sequences_input = pad_sequences(sequences_input, max_input_seq_length, padding='pre', truncating='post')\n",
        "  print('Printing sample sequences:')\n",
        "  print(sequences_input[0], ':', idxseq_to_vocabseq(sequences_input[0]))\n",
        "  print(sequences_input[1], ':', idxseq_to_vocabseq(sequences_input[1]))\n",
        "  sequences_output = []\n",
        "  with open(output_file, 'w') as f:\n",
        "    for i, line in enumerate(sequences_input):\n",
        "      output, _ = decode_sequence(np.array([line], dtype=np.int32))\n",
        "      # print(output)\n",
        "      sequences_output.append(output)\n",
        "      f.write(output + '\\n')\n",
        "  return sequences_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDXF82m_iVi2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_out = post_process(data_dir_shakespeare + 'test.modern.nltktok', data_dir_shakespeare + 'm1.test.out')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oS5u1k8dRTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test set BLEU\n",
        "! perl /gdrive/My\\ Drive/Deep\\ Learning\\ Project/Shakespearizing-Modern-English/code/main/multi-bleu.perl -lc /gdrive/My\\ Drive/Deep\\ Learning\\ Project/Shakespearizing-Modern-English/data/test.original.nltktok < /gdrive/My\\ Drive/Deep\\ Learning\\ Project/Shakespearizing-Modern-English/data/m1.test.out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBd25KsZ06jH",
        "colab_type": "text"
      },
      "source": [
        "**GAN Model #2**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-DALLwcY3FB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# GAN model\n",
        "encoder_ip = Input(shape=(None, ))\n",
        "embed = Embedding(input_dim=max_vocab_size, output_dim=192, weights=[embeddings], trainable=False)\n",
        "encoder_embed = embed(encoder_ip)\n",
        "encoder_lstm = LSTM(units=256, return_sequences=True, recurrent_dropout=0.2, dropout=0.2,\n",
        "                        kernel_initializer='he_normal', recurrent_activation='tanh', return_state=True)\n",
        "encoder, state_h, state_c = encoder_lstm(encoder_embed)\n",
        "encoder_states = [state_h, state_c]\n",
        "encoder = BatchNormalization()(encoder)\n",
        "\n",
        "decoder_ip = Input(shape=(None, ))\n",
        "decoder_embed = embed(decoder_ip)\n",
        "decoder_lstm = LSTM(units=256, recurrent_dropout=0.2, dropout=0.2, return_sequences=True,\n",
        "                        kernel_initializer='he_normal',recurrent_activation='tanh', return_state=True)\n",
        "decoder, _, _ = decoder_lstm(decoder_embed, initial_state=encoder_states)\n",
        "\n",
        "attention_dot = dot([decoder, encoder], axes=[2, 2])\n",
        "attention_softmax = Activation('softmax')\n",
        "attention = attention_softmax(attention_dot)\n",
        "context = dot([attention, encoder], axes=[2,1])\n",
        "decoder_combined_context = concatenate([context, decoder])\n",
        "decoder_combined_context = BatchNormalization()(decoder_combined_context)\n",
        "\n",
        "output_layer = Dense(max_vocab_size, activation='softmax')\n",
        "output = output_layer(decoder_combined_context)\n",
        "generator = tf.keras.Model([encoder_ip, decoder_ip], output)\n",
        "# takes english sentence as input, gives shakespeare translation\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X09uXD2kvemO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator_ip = Input(shape=(24, max_vocab_size))\n",
        "discriminator_dense = Dense(512, activation='relu')\n",
        "discriminator_output_layer = Dense(1, activation='sigmoid')\n",
        "discriminator_output = discriminator_output_layer(Flatten()(discriminator_dense(discriminator_ip)))\n",
        "discriminator = tf.keras.Model(discriminator_ip, discriminator_output)\n",
        "# takes english/shakespeare sentence, gives 0/1\n",
        "discriminator.summary()\n",
        "discriminator.compile(loss='binary_crossentropy',\n",
        "            optimizer='adam',\n",
        "            metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqKzS983yGAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For the combined model we will only train the generator\n",
        "# discriminator.trainable = False\n",
        "gen_op = generator([encoder_ip, decoder_ip])\n",
        "valid = discriminator_output_layer(Flatten()(discriminator_dense(gen_op)))\n",
        "combined = tf.keras.Model([encoder_ip, decoder_ip], valid)\n",
        "combined.layers[3].trainable = False\n",
        "combined.layers[4].trainable = False\n",
        "combined.layers[5].trainable = False\n",
        "opt = tf.keras.optimizers.Adam(lr=0.02)\n",
        "combined.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        "# takes english sentence, generates shakespeare sentence, determines validity\n",
        "combined.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccIqJuIJ0IuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train GAN model\n",
        "n_epochs = 20\n",
        "batch_size = 32\n",
        "n_iters = int((n_epochs * len(data['train'][0]) / batch_size))\n",
        "for it in range(n_iters):\n",
        "  shuffled_idx = np.arange(len(data['train'][0]))\n",
        "  np.random.shuffle(shuffled_idx)\n",
        "  batch_idx = shuffled_idx[it*batch_size:(it+1)*batch_size]\n",
        "  batch_enc_ip = tf.convert_to_tensor(data['train'][0][batch_idx])\n",
        "  batch_dec_ip = tf.convert_to_tensor(data['train'][1][batch_idx])\n",
        "  # train discriminator\n",
        "  # gives loss and acc\n",
        "  half_batch = int(len(batch_idx)/2)\n",
        "  random_hb = np.random.randint(0, len(shuffled_idx), half_batch)\n",
        "  batch_random_enc_ip = tf.convert_to_tensor(data['train'][0][random_hb])\n",
        "  batch_random_dec_ip = tf.convert_to_tensor(data['train'][1][random_hb])\n",
        "  batch_random_dec_op = tf.keras.utils.to_categorical(data['train'][2][random_hb], num_classes=max_vocab_size)\n",
        "  gen_out = generator([batch_random_enc_ip, batch_random_dec_ip])\n",
        "  loss_real = discriminator.train_on_batch(batch_random_dec_op, tf.ones(half_batch, 1))\n",
        "  loss_fake = discriminator.train_on_batch(gen_out, tf.zeros(half_batch, 1))\n",
        "  loss = (loss_real[0] + loss_fake[0]) / 2\n",
        "  # train generator\n",
        "  combined.layers[3].set_weights(discriminator_dense.get_weights())  \n",
        "  combined.layers[5].set_weights(discriminator_output_layer.get_weights()) \n",
        "  g_loss = combined.train_on_batch([batch_enc_ip, batch_dec_ip], tf.ones((len(batch_idx)), 1))\n",
        "  # if it % 10 == 0:\n",
        "  print('iter:', it, ', loss:', loss, 'g_loss:', g_loss[0], ', loss_real:', loss_real[0], ', loss_fake:', loss_fake[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW34iKqXgqy-",
        "colab_type": "text"
      },
      "source": [
        "**Seq2Seq Global Attention and Pointer Model with Original Repo Code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSB34tN5hEbo",
        "colab_type": "text"
      },
      "source": [
        "Change runtime to Python 2.0 and Tensorflow 1.0 for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4qjA9ikgpeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(main_dir + shakespeare_dir + '/code/main')\n",
        "os.mkdir('tmp')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4B641qShAHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! python mt_main.py preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnEI06E0hC58",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pointer model\n",
        "! python mt_main.py train 5 pointer_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FH1E9pFhRn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# seq2seq with global attention\n",
        "! python mt_main.py train 5 seq2seq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMLFfX2lhX0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# inference\n",
        "# use saved model\n",
        "! python mt_main.py test tmp/global_att_256/seq2seq5.ckpt greedy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oKZq1I61Zmr",
        "colab_type": "text"
      },
      "source": [
        "**Final Output Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjXeyht0167-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_final = tf.keras.models.load_model(data_dir_shakespeare + 'models/model_final_2')\n",
        "encoder_model = tf.keras.models.load_model(data_dir_shakespeare + 'models/encoder_model_2')\n",
        "decoder_model = tf.keras.models.load_model(data_dir_shakespeare + 'models/decoder_model_2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j4zC0otOBWz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.chdir(main_dir + img2poem_dir + 'code/src')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg9KI8SX1xBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate output poem for an image\n",
        "!python test.py \"../images/test.jpg\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhbkQnSI4z1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(filename='../images/test.jpg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erKBl9f-12tc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display English peom\n",
        "!cat test.jpg_poem.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-RPs_yr16Y5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text style transfer on English poem\n",
        "sample_out = post_process(main_dir + '/img2poem/code/src/' + 'test.jpg_poem.txt', main_dir + '/img2poem/code/src/' + 'test.poem.out')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqv5Ik4z2b4r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# display Shakespearean prose\n",
        "sample_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR77Q7cG03fa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}